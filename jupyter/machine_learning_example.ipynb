{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Using Cerebral Cortex with Machine Learning Tools\n",
    "\n",
    "**Scenario:** Classify the type of motion from a smartphone's accelerometer and gyroscope sensors.\n",
    "\n",
    "This is based on a kaggle competition and example: https://www.kaggle.com/morrisb/what-does-your-smartphone-know-about-you\n",
    "\n",
    "**Reference:**\n",
    "Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. _A Public Domain Dataset for Human Activity Recognition Using Smartphones_. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Cerebral Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from util.dependencies import *\n",
    "CC = Kernel(\"/home/md2k/cc_conf/\")\n",
    "from settings import USER_ID\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load both data and labels into datastreams\n",
    "These are then converted to Pandas Dataframes for the remainder of the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_datastream = CC.get_stream('Kaggle-Features')\n",
    "label_datastream = CC.get_stream('Kaggle-ActivityLabels')\n",
    "\n",
    "both_dataframe = both_datastream.to_pandas().data\n",
    "label_dataframe = label_datastream.to_pandas().data\n",
    "\n",
    "both_dataframe = both_dataframe.drop(['timestamp','localtime','version','user'], axis=1)\n",
    "label_dataframe = label_dataframe.drop(['timestamp','localtime','version','user'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of events in each activity class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataframe.groupby('Activity').size().reset_index(name='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for model training\n",
    "The required packages are imported into the notebook before transforming the data.  \n",
    "\n",
    "1. Data is cleaned to remove any subject specific information\n",
    "2. The data is scaled using the `StandardScaler`\n",
    "3. Principal Component Analysis (PCA) is utilized to identify the features with the most descriptive power\n",
    "4. Text labels are encoded, `LabelEncoder`, due to most ML algorithms not working well with text\n",
    "5. The test-train split is created for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create datasets\n",
    "tsne_data = both_dataframe.copy()\n",
    "data_data = tsne_data.pop('Data')\n",
    "subject_data = tsne_data.pop('subject')\n",
    "\n",
    "# Scale data\n",
    "tsne_data = StandardScaler().fit_transform(tsne_data)\n",
    "\n",
    "# Reduce dimensions (speed up)\n",
    "tsne_data = PCA(n_components=0.95, random_state=3).fit_transform(tsne_data)\n",
    "\n",
    "# Split the data\n",
    "label_encoded = LabelEncoder().fit_transform(label_dataframe.Activity)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tsne_data, label_encoded, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n",
    "\n",
    "A Gradient Boosting Machine (GBM) is trained using Gradient Boosting Decision Trees with the features identified through PCA. This example uses 50 boosted trees to identify the best model.  The resulting classification accuracy peaks at 0.959 when utilizing 500+ boosted trees; however, with only 50, this is still 0.937."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_estimators=50\n",
    "\n",
    "# Create the model\n",
    "lgbm = LGBMClassifier(n_estimators=number_of_estimators)\n",
    "lgbm = lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "score = accuracy_score(y_true=y_test, y_pred=lgbm.predict(X_test))\n",
    "print('Classification Accuracy:',score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
