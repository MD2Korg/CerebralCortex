{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Your Own Data\n",
    "mCerebrum is not the only want to collect and load data into *Cerebral Cortex*.  It is possible to import your own structured datasets into the platform. This example will demonstration the loading of existing data and subsequently how to read it back from Cerebral Cortex through the same mechanisms you have been utilizing.  Additionally, this example demonstrates how to write a custom data transformation fuction to manipulate this data and produce a smoothed result which it then visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from util.dependencies import *\n",
    "from settings import USER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Cerebral Cortex provides a set of predefined data import routines that fit typical use cases.  The most common is CSV data parser, `from cerebralcortex.data_importer.data_parsers import csv_data_parser`.  These parsers are easy to write and can be extended to support most types of data.  Additionally, the data importer, `import_data`, needs to be brought into this notebook so that we can start the data import process.\n",
    "\n",
    "The `import_data` method requires several parameters that are discussed below.\n",
    "- `cc_config`: The path to the configuration files for Cerebral Cortex.  This is the same folder that you would utilize for the `Kernel` initialization\n",
    "- `input_data_dir`: The path to where the data to be imported is located.  In this example, `sample_data` is available in the file/folder browser on the left and you should explore the files located inside of it.\n",
    "- `user_id`: This is the UUID that owns the data to be imported into the system.\n",
    "- `data_file_extension`: The type of files to be considered for import\n",
    "- `data_parser`: The import method or another that defines how to interpret the data samples on a per-line basis\n",
    "- `gen_report`: A True/False value that controls if a report is printed to the screen when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebralcortex.data_importer.data_parsers import csv_data_parser\n",
    "from cerebralcortex.data_importer import import_dir\n",
    "\n",
    "import_dir(\n",
    "    cc_config=\"/home/md2k/cc_conf/\",\n",
    "    input_data_dir=\"sample_data/\",\n",
    "    user_id=USER_ID,\n",
    "    data_file_extension=[\".csv\"],\n",
    "    data_parser=csv_data_parser,\n",
    "    gen_report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CerebralCortex object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = Kernel(\"/home/md2k/cc_conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iot_stream = CC.get_stream(\"iot-data-stream\")\n",
    "\n",
    "# Data\n",
    "iot_stream.show(truncate=False)\n",
    "\n",
    "# Metadata\n",
    "iot_stream.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to write an algorithm\n",
    "This section provides an example of how to write a simple smoothing algorithm and apply it to the data that was just imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Schema\n",
    "This schema defines what the computation module will return to the execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"smooth_vals\",  FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a user defined function\n",
    "The user defined function (UDF) is one of two mechanisms available for distributed data processing within the Apache Spark framework.  \n",
    "\n",
    "The `F.udf` Python decorator assigns the recently defined `schema` as a return type of the `udf` method.  The method, `smooth_algo` accepts a list of values, `vals`, and any python-based operations can be run over this data window to produce the data defined in the schema.  In this case, we are computing a simple windowed average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(schema)\n",
    "def smooth_algo(vals):\n",
    "    return [sum(vals)/len(vals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run smoothing algorithm on imported data\n",
    "The smoothing algorithm is applied to the datastream by calling the `run_algorithm` method and passing the method as a parameter along with which columns, `some_vals`, that should be sent to the algorithms.  Finally, the `windowDuration` parameter specified the size of the time windows on which to segment the data before applying the algorithm.  Notice that when the next cell is run, the operation completes nearly instantaneously.  This is due to the lazy evaluation aspects of the Spark framework.  When you run the next cell to show the data, the algorithm will be applied to the wholee dataset before showing the results on the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_stream = iot_stream.run_algorithm(smooth_algo, columnNames=[\"some_vals\"], windowDuration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_stream.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "These are two plots that show the original data and the smoothed data to visually check how the algorithm transforms the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iot_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_stream.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
