{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Your Own Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from util.dependencies import *\n",
    "from settings import USER_ID\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType, TimestampType, IntegerType\n",
    "from cerebralcortex.data_importer.data_parsers import csv_data_parser\n",
    "from cerebralcortex.data_importer import import_dir\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CSV/IoT data (demo stream name - demo-location-data-stream)\n",
    "import_dir(\n",
    "    cc_config=\"/home/md2k/cc_conf/\",\n",
    "    input_data_dir=\"sample_data/\",\n",
    "    user_id=USER_ID,\n",
    "    data_file_extension=[\".csv\"],\n",
    "    data_parser=csv_data_parser,\n",
    "    gen_report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CerebralCortex object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = Kernel(\"/home/md2k/cc_conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------+------------------------------------+\n",
      "|timestamp          |localtime          |some_vals          |version|user                                |\n",
      "+-------------------+-------------------+-------------------+-------+------------------------------------+\n",
      "|2019-01-09 17:35:00|2019-01-09 17:35:00|0.0851887269487499 |1      |00000000-afb8-476e-9872-6472b4e66b68|\n",
      "|2019-01-09 17:35:01|2019-01-09 17:35:01|0.16867549655743164|1      |00000000-afb8-476e-9872-6472b4e66b68|\n",
      "|2019-01-09 17:35:02|2019-01-09 17:35:02|0.7404850816560419 |1      |00000000-afb8-476e-9872-6472b4e66b68|\n",
      "|2019-01-09 17:35:03|2019-01-09 17:35:03|0.7131609970182962 |1      |00000000-afb8-476e-9872-6472b4e66b68|\n",
      "|2019-01-09 17:35:04|2019-01-09 17:35:04|0.24253081438369195|1      |00000000-afb8-476e-9872-6472b4e66b68|\n",
      "+-------------------+-------------------+-------------------+-------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_stream = CC.get_stream(\"iot-data-stream\")\n",
    "location_stream.show(5, truncate=False)\n",
    "#location_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"mean\" among (timestamp, localtime, some_vals, version, user);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o248.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"mean\" among (timestamp, localtime, some_vals, version, user);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:224)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:224)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:223)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1269)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1236)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9f22ec0b16d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#df.withColumn('d', fun_function(df.some_vals)).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# df2.groupBy(\"window\").agg(F.mean('avg(some_vals)')).show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \"\"\"\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"mean\" among (timestamp, localtime, some_vals, version, user);'"
     ]
    }
   ],
   "source": [
    "df=location_stream.data\n",
    "\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    #v = pdf.v\n",
    "    return pdf\n",
    "\n",
    "#location_stream.compute_max().show(truncate=False)\n",
    "#df.withColumn('d', fun_function(df.some_vals)).show()\n",
    "\n",
    "df.groupBy(\"timestamp\", window(\"timestamp\", \"2 seconds\")).agg(df[\"mean\"]).show()\n",
    "\n",
    "# df2.groupBy(\"window\").agg(F.mean('avg(some_vals)')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'iot-data-stream', 'description': 'this is a demo iot look a like stream', 'metadata_hash': '8eea2a10-96c6-3a17-a7a1-db87187b6b12', 'input_streams': [], 'annotations': [], 'data_descriptor': [<cerebralcortex.core.metadata_manager.stream.data_descriptor.DataDescriptor object at 0x7f7b834d1438>], 'modules': [<cerebralcortex.core.metadata_manager.stream.module_info.ModuleMetadata object at 0x7f7b834d14e0>], 'version': 1}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_stream.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a smoothing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ts\", TimestampType())\n",
    "    ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def smooth_df(data):\n",
    "#     df= data['some_vals'].ewm(span = 3600).mean()\n",
    "#     print(type(df))\n",
    "    df2 = pd.DataFrame([data.timestamp], columns=['ts'])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run smoothing algorithm on imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  ts|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#smooth_stream = location_stream.groupby(\"user\").compute(smooth_df)\n",
    "smooth_stream = location_stream.compute(smooth_df)\n",
    "smooth_stream.show(3 truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'stddev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-a1b2ae7d4e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#df.withColumn('d', fun_function(df.some_vals)).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5 minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"some_vals\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'stddev'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def fun_function(a):\n",
    "    return a+2\n",
    "df=location_stream.data\n",
    "\n",
    "#df.withColumn('d', fun_function(df.some_vals)).show()\n",
    "\n",
    "df.groupBy(\"timestamp\", window(\"timestamp\", \"5 minutes\")).stddev(\"some_vals\").show(5, False)\n",
    "\n",
    "interval = 60\n",
    "gdf = df.withColumn(\n",
    "    'time_interval',\n",
    "    pyspark.sql.functions.from_unixtime(pyspark.sql.functions.floor(pyspark.sql.functions.unix_timestamp(df['timestamp']) / interval) * interval)\n",
    ").groupBy('time_interval')\n",
    "\n",
    "#df.groupBy(second(\"timestamp\").alias(\"hour\")).agg(mean(\"some_vals\").alias(\"mean\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipyleaflet/leaflet.py:80: DeprecationWarning:\n",
      "\n",
      "Traits should be given as instances, not types (for example, `Int()`, not `Int`). Passing types is deprecated in traitlets 4.1.\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipyleaflet/leaflet.py:297: DeprecationWarning:\n",
      "\n",
      "metadata {'dtype': None} was set from the constructor. With traitlets 4.1, metadata should be set using the .tag() method, e.g., Int().tag(key1='value1', key2='value2')\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipyleaflet/leaflet.py:416: DeprecationWarning:\n",
      "\n",
      "metadata {'trait': <traitlets.traitlets.Instance object at 0x7f33837de320>} was set from the constructor. With traitlets 4.1, metadata should be set using the .tag() method, e.g., Int().tag(key1='value1', key2='value2')\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipyleaflet/leaflet.py:553: DeprecationWarning:\n",
      "\n",
      "Traits should be given as instances, not types (for example, `Int()`, not `Int`). Passing types is deprecated in traitlets 4.1.\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipyleaflet/leaflet.py:785: DeprecationWarning:\n",
      "\n",
      "Traits should be given as instances, not types (for example, `Int()`, not `Int`). Passing types is deprecated in traitlets 4.1.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3633bac250db4fc8be24e53c576c4af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attribution': 'Map â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                  ts|\n",
      "+----+--------------------+\n",
      "|17.0| 2018-03-10 15:27:18|\n",
      "|13.0| 2018-03-11 12:27:18|\n",
      "|25.0|2018-03-12 11:27:...|\n",
      "+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+----------------------+-------------------+\n",
      "|id   |ts                    |groupby_col        |\n",
      "+-----+----------------------+-------------------+\n",
      "|17.0 |2018-03-10 15:27:18   |2018-03-10-15-27-18|\n",
      "|13.0 |2018-03-11 12:27:18   |2018-03-11-12-27-18|\n",
      "|25.0 |2018-03-12 11:27:18.98|2018-03-12-11-27-18|\n",
      "|20.0 |2018-03-13 15:27:18   |2018-03-13-15-27-18|\n",
      "|17.0 |2018-03-14 12:27:18   |2018-03-14-12-27-18|\n",
      "|99.0 |2018-03-15 11:27:17.56|2018-03-15-11-27-17|\n",
      "|156.0|2018-03-22 11:27:18   |2018-03-22-11-27-18|\n",
      "|17.0 |2018-03-31 11:27:18   |2018-03-31-11-27-18|\n",
      "|25.0 |2018-03-15 11:27:18   |2018-03-15-11-27-18|\n",
      "|25.0 |2018-03-16 11:27:18   |2018-03-16-11-27-18|\n",
      "+-----+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = sparkSession.createDataFrame([(17.00, \"2018-03-10T15:27:18+00:00\"),\n",
    "                            (13.00, \"2018-03-11T12:27:18+00:00\"),\n",
    "                            (25.00, \"2018-03-12T11:27:18.98+00:00\"),\n",
    "                            (20.00, \"2018-03-13T15:27:18+00:00\"),\n",
    "                            (17.00, \"2018-03-14T12:27:18+00:00\"),\n",
    "                            (99.00, \"2018-03-15T11:27:17.56+00:00\"),\n",
    "                            (156.00, \"2018-03-22T11:27:18+00:00\"),\n",
    "                            (17.00, \"2018-03-31T11:27:18+00:00\"),\n",
    "                            (25.00, \"2018-03-15T11:27:18+00:00\"),\n",
    "                            (25.00, \"2018-03-16T11:27:18+00:00\")\n",
    "                            ],\n",
    "                           [\"id\", \"ts\"])\n",
    "df = df.withColumn('ts', df.ts.cast('timestamp'))\n",
    "df.show(3)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"ts\", TimestampType())\n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def some_udf(df):\n",
    "    # some computation\n",
    "    df = pd.DataFrame(([df[\"id\"], df[\"ts\"]]), columns=['id', 'ts'])\n",
    "    return df\n",
    "\n",
    "extended = (df.withColumn(\"groupby_col\", \n",
    "                          F.concat(F.col(\"ts\").cast(\"date\"), F.lit(\"-\"), F.hour(F.col(\"ts\")), F.lit(\"-\"), F.minute(F.col(\"ts\")), F.lit(\"-\"), F.second(F.col(\"ts\"))).cast(\"string\")\n",
    "                         ))\n",
    "extended.show(truncate=False)\n",
    "#df.groupby('id', F.window(\"ts\", \"15 days\")).apply(some_udf).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
